{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T02:41:32.434367Z",
     "start_time": "2020-06-09T02:41:30.993011Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T02:41:42.058970Z",
     "start_time": "2020-06-09T02:41:42.012063Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "### 数据初步分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:14:38.419713Z",
     "start_time": "2020-06-07T16:14:38.384254Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./data/titanic/train.csv\")\n",
    "data_test = pd.read_csv(\"./data/titanic/test.csv\")\n",
    "data_set = [data_train, data_test]\n",
    "\n",
    "print(data_train.info())\n",
    "print(data_train['Survived'].value_counts()/len(data_train))\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:14:42.288675Z",
     "start_time": "2020-06-07T16:14:42.280678Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PassengerId : 乘客的序号, 类别型变量\n",
    "- Pclass : 应该是仓位等级,可能跟身份地位挂钩, 类别型变量\n",
    "- Name : 姓名, 类别型变量\n",
    "- Age : 数值型连续变量\n",
    "- SibSp : 兄弟姐妹的数量, 类别\n",
    "- Parch : 父母或者孩子的数量, 类别\n",
    "- Ticket : 票的编号\n",
    "- Fare : 票价\n",
    "- Cabin : 仓位号 大量缺失\n",
    "- Embarked : 类别型变量 S, C, Q\n",
    "- Survived : 目标变量, 二分类\n",
    "\n",
    "接下来查看变量的分布情况:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:14:46.527886Z",
     "start_time": "2020-06-07T16:14:45.524556Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "data_col = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "for i, col in enumerate(data_col,1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    data_train[col].hist(bins=35, color='blue')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('frequent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看变量与目标变量之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:14:50.726534Z",
     "start_time": "2020-06-07T16:14:49.140048Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i, col in enumerate(data_col, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    data_train[data_train.Survived == 0][col].hist(bins=35, color='blue', alpha=0.5, \\\n",
    "                                                   label = f'{col} with Survive == 0')\n",
    "    data_train[data_train.Survived == 1][col].hist(bins=35, color='red', alpha=0.5, \\\n",
    "                                                   label = f'{col} with Survive == 1')\n",
    "    plt.legend()\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequent')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pclss 仓位的等级是一个影响因素, 可以看到Pclass的数值从小到大,存活率逐渐降低.\n",
    "- Sex 性别是一个影响因素, Lady First\n",
    "- Age 年龄越小或者越大应该有较大的存活率\n",
    "- SibSp 太多会团灭\n",
    "- Parch 为1或者2时有较高的存活率\n",
    "- Fare 票价, 有一个票价死亡率很高\n",
    "- Embarked 从C港登陆的存活率高与其他港"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失变量的分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:14:54.903177Z",
     "start_time": "2020-06-07T16:14:54.604446Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train.isnull().sum().plot(kind='bar')\n",
    "plt.xlabel('Train Set Null Count')\n",
    "plt.show()\n",
    "data_test.isnull().sum().plot(kind='bar')\n",
    "plt.xlabel('Test Set Null Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先分析Cabin, Cabin是一个类别型变量,并且应该有很多种类,所以无法进行填值.\n",
    "\n",
    "根据Cabin是否缺失可以构造出两个变量, 一种是有Cabin数据的, 另一种是没有Cabin.\n",
    "\n",
    "然后分析它们与Survived之间是否有联系."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:14:58.713427Z",
     "start_time": "2020-06-07T16:14:58.708473Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(data_train['Cabin'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:15:01.742508Z",
     "start_time": "2020-06-07T16:15:01.737370Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train['Cabin_Yes'] = data_train.Cabin.apply(lambda x: 1 if type(x) == str else 0)\n",
    "data_train['Cabin_No'] = data_train.Cabin.apply(lambda x: 0 if type(x) == str else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:15:02.532409Z",
     "start_time": "2020-06-07T16:15:02.520440Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:15:03.750755Z",
     "start_time": "2020-06-07T16:15:03.612598Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "data_train[data_train.Survived == 1]['Cabin_Yes'].hist(bins = 10, color = 'red', alpha = 0.5, \n",
    "                                                       label = f'Survived = 1')\n",
    "data_train[data_train.Survived == 0]['Cabin_Yes'].hist(bins = 10, color = 'blue', alpha = 0.5, \n",
    "                                                       label = f'Survived = 0')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以直观的看出, 有船舱的存活率要高一些\n",
    "\n",
    "票的等级是否和有无船舱有联系?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:15:12.112322Z",
     "start_time": "2020-06-07T16:15:11.975661Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "data_train[data_train.Cabin_Yes == 1]['Pclass'].hist(bins = 10, color = 'red', label = 'Cabin Yes', alpha = 0.5)\n",
    "data_train[data_train.Cabin_Yes == 0]['Pclass'].hist(bins = 10, color = 'blue', label = 'Cabin No', alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Pclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个等级都或多或少有缺失船舱的现象, 每个等级也均有船舱, 这个数据应该是一个干扰数据,\n",
    "因为有船舱的数据的乘客大多是Pclass = 1的乘客, 因此显得存活率比较高. \n",
    "所以, 我们将船舱数据给丢弃."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:15:18.034594Z",
     "start_time": "2020-06-07T16:15:18.012151Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in ['Cabin_Yes', 'Cabin_No']:\n",
    "    if col in data_train.columns:\n",
    "        data_train.drop(columns = [col], inplace = True)    \n",
    "\n",
    "# 训练集, 测试集 统一丢弃 Cabin\n",
    "for data in data_set:\n",
    "    if 'Cabin' in data.columns:\n",
    "        data.drop(columns = ['Cabin'], inplace = True)\n",
    "\n",
    "print(data_train.columns, data_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来要拟合的数据是Age数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将数据用Name中的title分组, 然后再用每个title的平均值,对缺失年龄数据进行填值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:16.411318Z",
     "start_time": "2020-06-07T16:16:16.403334Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "TitleMap = defaultdict(int)\n",
    "TitleAge = defaultdict(int)\n",
    "\n",
    "title_list = []\n",
    "\n",
    "if not TitleMap:\n",
    "    for name in data_train['Name']:\n",
    "        title = re.match(\".*[,](.*?)[.].*\", name).group(1).strip()\n",
    "        TitleMap[title] += 1\n",
    "        title_list.append(title)\n",
    "        \n",
    "df_title = pd.DataFrame(columns = ['Title'], data = title_list)\n",
    "train_dummies = pd.concat([data_train, df_title], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:17.837501Z",
     "start_time": "2020-06-07T16:16:17.198332Z"
    }
   },
   "outputs": [],
   "source": [
    "for title in train_dummies['Title']:\n",
    "    TitleAge[title] = train_dummies[train_dummies.Title == title]['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:19.422043Z",
     "start_time": "2020-06-07T16:16:19.358701Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, title in enumerate(train_dummies['Title']):\n",
    "    if pd.isnull(train_dummies.loc[idx, 'Age']):\n",
    "        train_dummies.loc[idx, 'Age'] = TitleAge[title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:20.284569Z",
     "start_time": "2020-06-07T16:16:20.147386Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dummies['Age'].hist(bins = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:21.270441Z",
     "start_time": "2020-06-07T16:16:21.123317Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train['Age'].hist(bins = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:22.849803Z",
     "start_time": "2020-06-07T16:16:22.843214Z"
    }
   },
   "outputs": [],
   "source": [
    "test_title = []\n",
    "\n",
    "for name in data_test['Name']:\n",
    "    title = re.match(\".*[,](.*?)[.].*\", name).group(1).strip()\n",
    "    test_title.append(title)\n",
    "\n",
    "df_title = pd.DataFrame(columns = ['Title'], data = test_title)\n",
    "test_dummies = pd.concat([data_test, df_title], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:24.815819Z",
     "start_time": "2020-06-07T16:16:24.781762Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, title in enumerate(test_dummies['Title']):\n",
    "    if pd.isnull(test_dummies.loc[idx, 'Age']) and title in TitleAge:\n",
    "        test_dummies.loc[idx, 'Age'] = TitleAge[title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:25.693198Z",
     "start_time": "2020-06-07T16:16:25.689698Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = train_dummies\n",
    "data_test = test_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练集中港口还有缺失数据, 我们用常用港口'S' 来填补\n",
    "在测试集中缺失了一个票价, 我们用平均票价来填补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:27.786078Z",
     "start_time": "2020-06-07T16:16:27.779093Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train['Embarked'].fillna(value = 'S', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:28.995383Z",
     "start_time": "2020-06-07T16:16:28.989438Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集中还缺失了一个票价数据, 我们根据训练集中得不同Pclass对应得票价来填充数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:30.428455Z",
     "start_time": "2020-06-07T16:16:30.416494Z"
    }
   },
   "outputs": [],
   "source": [
    "MeanFare = defaultdict(int)\n",
    "for i in range(1, 4):\n",
    "    MeanFare[i] = data_train[data_train.Pclass == i]['Fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:31.100786Z",
     "start_time": "2020-06-07T16:16:31.095799Z"
    }
   },
   "outputs": [],
   "source": [
    "fare_na = data_test[pd.isna(data_test.Fare)]['Fare'].index\n",
    "for index in fare_na:\n",
    "    data_test.loc[index, 'Fare'] = MeanFare[data_test.loc[index, 'Pclass']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:31.755555Z",
     "start_time": "2020-06-07T16:16:31.749569Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:45:35.312265Z",
     "start_time": "2020-05-28T16:45:35.093127Z"
    }
   },
   "source": [
    "至此, 缺失数据全部处理完毕\n",
    "### 数据深入分析\n",
    "首先丢弃一些无用项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:35.535717Z",
     "start_time": "2020-06-07T16:16:35.517782Z"
    }
   },
   "outputs": [],
   "source": [
    "# 丢弃 passengerId, Name, Ticket\n",
    "train_dummies = data_train.drop(columns = ['PassengerId', 'Name', 'Ticket'])\n",
    "train_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:36.183829Z",
     "start_time": "2020-06-07T16:16:36.172826Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dummies = data_test.drop(columns = ['PassengerId', 'Name', 'Ticket'])\n",
    "test_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展开 Pclass 以及 Embarked数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:40.719786Z",
     "start_time": "2020-06-07T16:16:40.703356Z"
    }
   },
   "outputs": [],
   "source": [
    "train_onehot = pd.get_dummies(train_dummies, columns = ['Pclass', 'Embarked', 'Sex'])\n",
    "test_onehot = pd.get_dummies(test_dummies, columns = ['Pclass', 'Embarked', 'Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T16:16:42.852648Z",
     "start_time": "2020-06-07T16:16:42.839683Z"
    }
   },
   "outputs": [],
   "source": [
    "train_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:36.106990Z",
     "start_time": "2020-05-31T07:19:35.492052Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_matrix = train_onehot.corr()\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(corr_matrix, annot = True, fmt=\".2f\", cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:38.913297Z",
     "start_time": "2020-05-31T07:19:38.766526Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "corr_matrix['Survived'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "性别 仓位等级 登陆港口 票价 对 是否存活都有一定的影响力\n",
    "\n",
    "接下来的问题是, 怎么处理Age 以及 SibSp Parch 这三个数据?\n",
    "\n",
    "首先分析Age和Survived之间的关系:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:42.288543Z",
     "start_time": "2020-05-31T07:19:42.048081Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "train_onehot[train_onehot.Survived == 1]['Age'].hist(bins = 35, color = 'red', alpha = 0.5, label = 'Survived = 1')\n",
    "train_onehot[train_onehot.Survived == 0]['Age'].hist(bins = 35, color = 'blue', alpha = 0.5, label = 'Survived = 0')\n",
    "plt.legend()\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小孩子的存活率貌似较高, 30-40岁的存活率较低, 可能是青年男子\n",
    "\n",
    "方案一 : 将年龄分组 (0, 15] child and not child (15, ...)\n",
    "\n",
    "方案二 : 标准化, 首先测试这个方案的效果.\n",
    "\n",
    "接下来处理 SibSp 和 Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:45.187313Z",
     "start_time": "2020-05-31T07:19:44.742544Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in ['SibSp', 'Parch']:\n",
    "    plt.figure(figsize=(8,8))\n",
    "    train_onehot[train_onehot.Survived == 0][col].hist(bins=35, color='blue', alpha=0.5, \\\n",
    "                                                   label = f'{col} with Survive == 0')\n",
    "    train_onehot[train_onehot.Survived == 1][col].hist(bins=35, color='red', alpha=0.5, \\\n",
    "                                                   label = f'{col} with Survive == 1')\n",
    "    plt.legend()\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequent')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 SibSp 与 Parch 相加为FamilySize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:48.009886Z",
     "start_time": "2020-05-31T07:19:47.990177Z"
    }
   },
   "outputs": [],
   "source": [
    "train_family = []\n",
    "for i in range(len(train_onehot)):\n",
    "    train_family.append(train_onehot.loc[i, 'SibSp'] + train_onehot.loc[i, 'Parch'])\n",
    "\n",
    "df_family = pd.DataFrame(columns = ['FamilySize'], data = train_family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:49.147888Z",
     "start_time": "2020-05-31T07:19:49.143936Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dummies = pd.concat([train_onehot, df_family], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:50.410799Z",
     "start_time": "2020-05-31T07:19:50.130621Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "train_dummies[train_dummies.Survived == 1]['FamilySize'].hist(bins = 35, color = 'red', alpha = 0.5, label = 'Survived = 1')\n",
    "train_dummies[train_dummies.Survived == 0]['FamilySize'].hist(bins = 35, color = 'blue', alpha = 0.5, label = 'Survived = 0')\n",
    "plt.legend()\n",
    "plt.xlabel('FamilySize')\n",
    "plt.ylabel('freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试将FamilySize 分组\n",
    "alone 0\n",
    "Moderate 1-3\n",
    "Large > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:51.495576Z",
     "start_time": "2020-05-31T07:19:51.490589Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dummies['FamilySize'] = train_dummies['FamilySize'].map(lambda x: 'alone' if x == 0 else ('Moderate' if x <= 3 else 'Large'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:52.516298Z",
     "start_time": "2020-05-31T07:19:52.508960Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dummies = pd.get_dummies(data=train_dummies, columns = ['FamilySize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:53.506134Z",
     "start_time": "2020-05-31T07:19:53.493169Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T13:04:02.339071Z",
     "start_time": "2020-05-30T13:04:01.351513Z"
    }
   },
   "source": [
    "对测试集做同样的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:19:54.783071Z",
     "start_time": "2020-05-31T07:19:54.764383Z"
    }
   },
   "outputs": [],
   "source": [
    "test_family = []\n",
    "for i in range(len(test_onehot)):\n",
    "    test_family.append(test_onehot.loc[i, 'SibSp'] + test_onehot.loc[i, 'Parch'])\n",
    "\n",
    "df_family_test = pd.DataFrame(columns = ['FamilySize'], data = test_family)\n",
    "\n",
    "test_dummies = pd.concat([test_onehot, df_family_test], axis = 1)\n",
    "\n",
    "test_dummies['FamilySize'] = test_dummies['FamilySize'].map(lambda x: 'alone' if x == 0 else ('Moderate' if x <= 3 else 'Large'))\n",
    "\n",
    "test_dummies = pd.get_dummies(data=test_dummies, columns = ['FamilySize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:20:07.552637Z",
     "start_time": "2020-05-31T07:20:07.544629Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = train_dummies.drop(columns = ['SibSp', 'Parch', 'Title'])\n",
    "data_test = test_dummies.drop(columns = ['SibSp', 'Parch', 'Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:20:08.430093Z",
     "start_time": "2020-05-31T07:20:08.418089Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T13:11:02.346550Z",
     "start_time": "2020-05-30T13:11:02.326563Z"
    }
   },
   "source": [
    "性别, 票的等级, 票价 一定程度上和Title有挂钩, 所以这里我们为了防止模型臃肿, 将Title丢弃\n",
    "\n",
    "至此, 我们以及完成了数据预处理步骤, 接下来将模型投入训练."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "首先将票价以及年龄进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:20:10.264813Z",
     "start_time": "2020-05-31T07:20:10.258828Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "data_train[['Age', 'Fare']] = sc.fit_transform(data_train[['Age', 'Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:20:11.677539Z",
     "start_time": "2020-05-31T07:20:11.671820Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test[['Age', 'Fare']] = sc.transform(data_test[['Age', 'Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:20:12.628814Z",
     "start_time": "2020-05-31T07:20:12.624792Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test['Fare'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:22:09.751665Z",
     "start_time": "2020-05-31T02:22:09.596638Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train = data_train.drop(columns = ['Survived'])\n",
    "y_train = data_train['Survived']\n",
    "X_test = data_test\n",
    "LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:22:12.937190Z",
     "start_time": "2020-05-31T02:22:12.856523Z"
    }
   },
   "outputs": [],
   "source": [
    "LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:22:14.577406Z",
     "start_time": "2020-05-31T02:22:14.561144Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:22:15.644884Z",
     "start_time": "2020-05-31T02:22:15.621153Z"
    }
   },
   "outputs": [],
   "source": [
    "sum((pred == 1)) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:22:18.528055Z",
     "start_time": "2020-05-31T02:22:18.506765Z"
    }
   },
   "outputs": [],
   "source": [
    "Test_passengerId = pd.read_csv('./data/titanic/test.csv')['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:22:20.341764Z",
     "start_time": "2020-05-31T02:22:20.281607Z"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'PassengerId':Test_passengerId, 'Survived':pred.astype(np.int32)})\n",
    "result.to_csv(\"./data/titanic/gender_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:22:23.878107Z",
     "start_time": "2020-05-31T02:22:23.576612Z"
    }
   },
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(columns = list(X_train.columns), data = LR.coef_)\n",
    "coef.plot(kind = 'bar', figsize = (12,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline取得了0.75119的分数\n",
    "\n",
    "还不错毕竟只是随便用LR拟合的模型\n",
    "\n",
    "\n",
    "### 在训练集中, 我们进行交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:25:30.502510Z",
     "start_time": "2020-05-31T02:25:30.092163Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'C' : [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "    'solver' : ['liblinear'],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LR, params, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:25:31.622668Z",
     "start_time": "2020-05-31T02:25:31.612529Z"
    }
   },
   "outputs": [],
   "source": [
    "print(grid.best_score_, grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:25:33.302026Z",
     "start_time": "2020-05-31T02:25:33.282431Z"
    }
   },
   "outputs": [],
   "source": [
    "LR.set_params(**grid.best_params_)\n",
    "LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:26:09.935786Z",
     "start_time": "2020-05-31T02:26:09.912490Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = LR.predict(X_test)\n",
    "sum((pred == 1)) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T02:26:11.398308Z",
     "start_time": "2020-05-31T02:26:11.377714Z"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'PassengerId':Test_passengerId, 'Survived':pred.astype(np.int32)})\n",
    "result.to_csv(\"./data/titanic/gender_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.75598\n",
    "\n",
    "略有进步\n",
    "\n",
    "引入多种分类器 查看效果,\n",
    "\n",
    "树模型 : 决策树, Adaboost, GBDT, XGBoost, RF\n",
    "\n",
    "SVM, LR, KNN, GPC, NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:02:54.052855Z",
     "start_time": "2020-05-31T08:02:54.040913Z"
    }
   },
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "        self.best = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.best[key] = {'score':gs.best_score_, 'params':gs.best_params_}\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]\n",
    "    def best_params(self):\n",
    "        return self.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:02:55.119002Z",
     "start_time": "2020-05-31T08:02:55.113980Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:11:19.736960Z",
     "start_time": "2020-05-31T08:11:19.727986Z"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression' : LogisticRegression(),\n",
    "    'Decision Tree' : DecisionTreeClassifier(),\n",
    "    'AdaBoost' : AdaBoostClassifier(),\n",
    "    'GradientBoost' : GradientBoostingClassifier(),\n",
    "    'RandomForest' : RandomForestClassifier(),\n",
    "    'XGBoost' : XGBClassifier(),\n",
    "    'SVC' : SVC(),\n",
    "    'KNN' : KNeighborsClassifier(),\n",
    "    'GPC' : GaussianProcessClassifier(),\n",
    "    'NB' : GaussianNB(),\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'Logistic Regression' : { 'penalty' : ['l1', 'l2'], 'C' : [0.5, 1.0, 2.0, 5.0, 10.0], 'solver' : ['liblinear'],},\n",
    "    'Decision Tree' : {'criterion' : ['gini', 'entropy'], 'max_depth' : [2, 3, 4, 5], },\n",
    "    'AdaBoost' : {'n_estimators' : [25, 50, 75,100], 'learning_rate' : [0.5, 1.0], },\n",
    "    'GradientBoost' : {'n_estimators' : [50, 75,100,125], 'learning_rate' : [0.1, 0.3, 1.0], 'max_depth' : [2, 3, 4, 5]},\n",
    "    'RandomForest' : {'n_estimators' : [25, 50, 75,100], 'criterion' : ['gini', 'entropy'], 'max_depth' : [2, 3, 4, 5]},\n",
    "    'XGBoost' : {'n_estimators' : [25, 50,75, 100], 'learning_rate' : [0.1, 0.3, 1.0], 'max_depth' : [2, 3, 4, 5]},\n",
    "    'SVC' : [ {'kernel' : ['rbf'], 'C' : [1.0, 2.0], 'gamma': [0.001, 0.005]},\n",
    "             {'kernel' : ['linear'], 'C' : [1.0, 2.0] }],\n",
    "    'KNN' : {'n_neighbors' : [5, 10, 20, 30]},\n",
    "    'GPC' : {'n_restarts_optimizer' : [5]},\n",
    "    'NB' : {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:11:49.130943Z",
     "start_time": "2020-05-31T08:11:20.856966Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Helper = EstimatorSelectionHelper(models, params)\n",
    "Helper.fit(X_train, y_train, cv=5)\n",
    "Helper.score_summary(sort_by='mean_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:12:40.798418Z",
     "start_time": "2020-05-31T08:12:40.792433Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params = Helper.best\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "丢弃分类器 NB KNN Decision Tree\n",
    "\n",
    "保留 LR, AdaB, GDBT, XGB, RF, SVC, GPC 共 7 个 分类器\n",
    "\n",
    "给它们分配上最优超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:34:21.567060Z",
     "start_time": "2020-05-31T08:34:21.563060Z"
    }
   },
   "outputs": [],
   "source": [
    "best_models = [\n",
    "    ('RandomForest',RandomForestClassifier()),\n",
    "    ('GradientBoost',GradientBoostingClassifier()),\n",
    "    ('AdaBoost',AdaBoostClassifier()),\n",
    "    ('XGBoost',XGBClassifier()),\n",
    "    ('SVC',SVC()),\n",
    "    ('Logistic Regression',LogisticRegression()),\n",
    "     ('GPC', GaussianProcessClassifier()),\n",
    "#     ('Decision Tree', DecisionTreeClassifier()),\n",
    "#      ('KNN', KNeighborsClassifier()),\n",
    "]\n",
    "\n",
    "for item in best_models:\n",
    "    key, clf = item\n",
    "    clf.set_params(**best_params[key]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:34:23.756140Z",
     "start_time": "2020-05-31T08:34:23.751154Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vote_hard = VotingClassifier(estimators = best_models , voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:34:27.159767Z",
     "start_time": "2020-05-31T08:34:25.788119Z"
    }
   },
   "outputs": [],
   "source": [
    "vote_hard.fit(X_train, y_train)\n",
    "vote_soft.fit(X_train, y_train)\n",
    "vote_hard_pred = vote_hard.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:34:29.217094Z",
     "start_time": "2020-05-31T08:34:29.211109Z"
    }
   },
   "outputs": [],
   "source": [
    "sum((vote_hard_pred == 1)) / len(vote_hard_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析分类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:34:31.613678Z",
     "start_time": "2020-05-31T08:34:31.536884Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred_train = vote_hard.predict(X_train)\n",
    "cr = classification_report(y_train, pred_train)\n",
    "print(cr)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_train, pred_train)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:34:34.105088Z",
     "start_time": "2020-05-31T08:34:34.098592Z"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'PassengerId':Test_passengerId, 'Survived':vote_hard_pred.astype(np.int32)})\n",
    "result.to_csv(\"./data/titanic/gender_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果是0.79904, 有一定的进步\n",
    "\n",
    "### 绘制学习曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:35:06.313970Z",
     "start_time": "2020-05-31T08:34:39.932624Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, \n",
    "                        train_sizes=np.linspace(.1, 1., 10), verbose=0, plot=True):\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.xlabel(\"Number of Train Samples\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                         alpha=0.1, color=\"b\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                         alpha=0.1, color=\"r\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Score in Train Set\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Score in Val Set\")\n",
    "    \n",
    "        plt.legend(loc=\"best\")\n",
    "        \n",
    "        plt.draw()\n",
    "        plt.gca().invert_yaxis()\n",
    "    \n",
    "    midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2\n",
    "    diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])\n",
    "    return midpoint, diff\n",
    "\n",
    "plot_learning_curve(vote_hard, \"Learning Curve\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 从学习曲线来看, 结果有一定的偏差, 应该尝试挖掘更多的代表性特征."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析Badcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:37:36.248694Z",
     "start_time": "2020-05-31T06:37:36.244708Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_data = train_dummies.drop(columns = ['SibSp', 'Parch', 'Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:37:37.202612Z",
     "start_time": "2020-05-31T06:37:37.192102Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:37:38.376417Z",
     "start_time": "2020-05-31T06:37:38.372428Z"
    }
   },
   "outputs": [],
   "source": [
    "Badcase = tmp_data[tmp_data.Survived != pred_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Badcase的数据分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:37:44.755932Z",
     "start_time": "2020-05-31T06:37:42.092799Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i, col in enumerate(Badcase.columns, 1):\n",
    "    plt.subplot(5, 3, i)\n",
    "    Badcase[Badcase.Survived == 1][col].hist(bins = 35, color = 'red', label = 'Survived = 1', alpha = 0.5)\n",
    "    Badcase[Badcase.Survived == 0][col].hist(bins = 35, color = 'blue', label = 'Survived = 0', alpha = 0.5)\n",
    "    plt.legend()\n",
    "    plt.xlabel(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:40:35.526391Z",
     "start_time": "2020-05-31T06:40:34.537637Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_corr = Badcase.corr()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(bad_corr, annot = True, fmt=\".2f\", cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:40:38.522899Z",
     "start_time": "2020-05-31T06:40:38.319703Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_corr = tmp_data.corr()\n",
    "plt.figure(figsize=(12,8))\n",
    "bad_corr['Survived'].plot(kind = 'bar', color = 'red', label = 'Bad_case', alpha = 0.5)\n",
    "tmp_corr['Survived'].plot(kind = 'bar', color = 'blue', label = 'All_case', alpha = 0.5)\n",
    "plt.title('Bad Case Vs All Case')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较反常的地方是\n",
    "\n",
    "- Age属性, Age与存活正相关\n",
    "- Embarked属性很诡异, 感觉可以将其去掉\n",
    "- Sex_male与Sex_female完全倒了过来, 训练集的特征 对 sex_male不利\n",
    "- FamilySize也是相反的情况\n",
    "\n",
    "感觉要构造一些新的特征, 或者要对特征做一下特殊的处理.\n",
    "\n",
    "首先, 我们将港口数据给去除\n",
    "\n",
    "然后中年男子也有存活率, 所以我们将年龄分组, 分为青中老\n",
    "\n",
    "Pclass 基本符合, 故不做处理\n",
    "\n",
    "有大量得存活男性, 被分错, 一会具体分析下原因."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:42:00.297874Z",
     "start_time": "2020-05-31T06:42:00.285875Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_v1 = X_train.drop(columns = ['Embarked_C', 'Embarked_Q', 'Embarked_S'])\n",
    "X_test_v1 = X_test.drop(columns = ['Embarked_C', 'Embarked_Q', 'Embarked_S'])\n",
    "X_train_v1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去掉Embarked特征\n",
    "\n",
    "结果反而变差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:44:53.730770Z",
     "start_time": "2020-05-31T06:44:52.973593Z"
    }
   },
   "outputs": [],
   "source": [
    "vote_hard.fit(X_train_v1, y_train)\n",
    "pred_v1 = vote_hard.predict(X_train_v1)\n",
    "cr = classification_report(y_train, pred_v1)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:43:48.369435Z",
     "start_time": "2020-05-31T06:43:48.363419Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred=pred_v1, y_true=y_train)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 年龄离散化分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:52:29.674974Z",
     "start_time": "2020-05-31T06:52:29.662060Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_v1[['Age', 'Fare']] = sc.inverse_transform(X_train_v1[['Age', 'Fare']])\n",
    "X_train_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:52:35.946132Z",
     "start_time": "2020-05-31T06:52:35.923235Z"
    }
   },
   "outputs": [],
   "source": [
    "Age_map = []\n",
    "for idx in range(len(X_train_v1)):\n",
    "    if X_train_v1.loc[idx, 'Age'] <= 15:\n",
    "        Age_map.append('Child')\n",
    "    elif X_train_v1.loc[idx, 'Age'] <= 30:\n",
    "        Age_map.append('Young')\n",
    "    elif X_train_v1.loc[idx, 'Age'] <= 50:\n",
    "        Age_map.append('Middle')\n",
    "    else:\n",
    "        Age_map.append('Old')\n",
    "\n",
    "Age_bin = pd.DataFrame(columns = ['Age_bin'], data = Age_map)\n",
    "\n",
    "X_train_v1 = pd.concat([X_train_v1, Age_bin], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:52:37.460265Z",
     "start_time": "2020-05-31T06:52:37.450304Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:52:41.138045Z",
     "start_time": "2020-05-31T06:52:41.129532Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_v2 = pd.get_dummies(data = X_train_v1.drop(columns = ['Age']), columns = ['Age_bin'])\n",
    "X_train_v2[['Fare']] = X_train[['Fare']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:52:43.632669Z",
     "start_time": "2020-05-31T06:52:43.622694Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:52:49.172688Z",
     "start_time": "2020-05-31T06:52:48.415147Z"
    }
   },
   "outputs": [],
   "source": [
    "vote_hard.fit(X_train_v2, y_train)\n",
    "pred_v2 = vote_hard.predict(X_train_v2)\n",
    "cr = classification_report(y_train, pred_v2)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T06:52:53.892919Z",
     "start_time": "2020-05-31T06:52:53.887641Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_true=y_train, y_pred=pred_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相较于第一个结果 变差了不少\n",
    "\n",
    "### 尝试加入新的特征 Ttile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:44.099501Z",
     "start_time": "2020-05-31T07:41:42.732922Z"
    }
   },
   "outputs": [],
   "source": [
    "for title in TitleMap.keys():\n",
    "    train_dummies[train_dummies.Title == title]['Survived'].value_counts().plot(kind = 'bar')\n",
    "    plt.xlabel(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细看了看 Title, 前面的分析看来是错误的.\n",
    "\n",
    "即使是相同的性别下, 不同的title存活率也不相同, 这个特征更加的详细, 那么增加这个特征看看效果."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:46.601299Z",
     "start_time": "2020-05-31T07:41:46.596310Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "train_backup = copy.deepcopy(train_dummies)\n",
    "train_backup['Title'] = train_dummies['Title'].map(lambda x : 'Rare' if TitleMap[x] <= 10 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:47.585045Z",
     "start_time": "2020-05-31T07:41:47.579015Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.unique(train_backup['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:48.529802Z",
     "start_time": "2020-05-31T07:41:48.521823Z"
    }
   },
   "outputs": [],
   "source": [
    "train_backup = pd.get_dummies(columns = ['Title'], data = train_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:49.482963Z",
     "start_time": "2020-05-31T07:41:49.469997Z"
    }
   },
   "outputs": [],
   "source": [
    "train_backup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对测试集做同样的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:51.590403Z",
     "start_time": "2020-05-31T07:41:51.582762Z"
    }
   },
   "outputs": [],
   "source": [
    "test_backup = copy.deepcopy(test_dummies)\n",
    "test_backup['Title'] = test_dummies['Title'].map(lambda x : 'Rare' if TitleMap[x] <= 10 else x)\n",
    "pd.unique(test_backup['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:52.631529Z",
     "start_time": "2020-05-31T07:41:52.624547Z"
    }
   },
   "outputs": [],
   "source": [
    "test_backup = pd.get_dummies(columns = ['Title'], data = test_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:53.590003Z",
     "start_time": "2020-05-31T07:41:53.584018Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    train_backup = train_backup.drop(columns = ['SibSp', 'Parch'])\n",
    "    test_backup = test_backup.drop(columns = ['SibSp', 'Parch'])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:55.699058Z",
     "start_time": "2020-05-31T07:41:55.694039Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trian_backup = train_backup.drop(columns = ['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:56.700330Z",
     "start_time": "2020-05-31T07:41:56.694656Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trian_backup[['Age', 'Fare']] = sc.fit_transform(X_trian_backup[['Age', 'Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:57.675670Z",
     "start_time": "2020-05-31T07:41:57.669687Z"
    }
   },
   "outputs": [],
   "source": [
    "test_backup[['Age', 'Fare']] = sc.transform(test_backup[['Age', 'Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:41:59.870150Z",
     "start_time": "2020-05-31T07:41:58.938795Z"
    }
   },
   "outputs": [],
   "source": [
    "vote_hard.fit(X_trian_backup, y_train)\n",
    "pred_backup = vote_hard.predict(X_trian_backup)\n",
    "cr = classification_report(y_train, pred_backup)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:42:10.623943Z",
     "start_time": "2020-05-31T07:42:10.617957Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred = pred_backup, y_true = y_train)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:37:51.378887Z",
     "start_time": "2020-05-31T07:37:50.594525Z"
    }
   },
   "outputs": [],
   "source": [
    "vote_hard.fit(X_trian_backup, y_train)\n",
    "pred_backup = vote_hard.predict(X_trian_backup)\n",
    "cr = classification_report(y_train, pred_backup)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:38:02.649425Z",
     "start_time": "2020-05-31T07:38:02.643436Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred = pred_backup, y_true = y_train)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_backup = {\n",
    "    'Logistic Regression' : LogisticRegression(),\n",
    "    'Decision Tree' : DecisionTreeClassifier(),\n",
    "    'AdaBoost' : AdaBoostClassifier(),\n",
    "    'GradientBoost' : GradientBoostingClassifier(),\n",
    "    'RandomForest' : RandomForestClassifier(),\n",
    "    'XGBoost' : XGBClassifier(),\n",
    "    'SVC' : SVC(),\n",
    "    'KNN' : KNeighborsClassifier(),\n",
    "    'GPC' : GaussianProcessClassifier(),\n",
    "    'NB' : GaussianNB(),\n",
    "}\n",
    "\n",
    "params_backup = {\n",
    "    'Logistic Regression' : { 'penalty' : ['l1', 'l2'], 'C' : [0.5, 1.0, 2.0, 5.0, 10.0], 'solver' : ['liblinear'],},\n",
    "    'Decision Tree' : {'criterion' : ['gini', 'entropy'], 'max_depth' : [2,4,6], },\n",
    "    'AdaBoost' : {'n_estimators' : [25, 50, 75,100], 'learning_rate' : [0.5, 1.0], },\n",
    "    'GradientBoost' : {'n_estimators' : [25, 50, 75,100], 'learning_rate' : [0.1, 0.3, 1.0], 'max_depth' : [2, 4, 6]},\n",
    "    'RandomForest' : {'n_estimators' : [25, 50, 75,100], 'criterion' : ['gini', 'entropy'], 'max_depth' : [2, 4, 6]},\n",
    "    'XGBoost' : {'n_estimators' : [25, 50,75, 100], 'learning_rate' : [0.1, 0.3, 1.0], 'max_depth' : [2, 4, 6]},\n",
    "    'SVC' : [ {'kernel' : ['rbf'], 'C' : [1.0, 2.0]},\n",
    "             {'kernel' : ['linear'], 'C' : [1.0, 2.0], 'gamma': [0.001, 0.0001] }],\n",
    "    'KNN' : {'n_neighbors' : [5, 10, 20, 30]},\n",
    "    'GPC' : {'n_restarts_optimizer' : [5]},\n",
    "    'NB' : {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:56:57.851877Z",
     "start_time": "2020-05-31T07:56:34.045214Z"
    }
   },
   "outputs": [],
   "source": [
    "Helper_backup = EstimatorSelectionHelper(models, params)\n",
    "Helper_backup.fit(X_trian_backup, y_train, cv=5)\n",
    "Helper_backup.score_summary(sort_by='mean_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:57:09.368110Z",
     "start_time": "2020-05-31T07:57:09.363125Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params_backup = Helper_backup.best\n",
    "best_params_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:57:26.039104Z",
     "start_time": "2020-05-31T07:57:26.034118Z"
    }
   },
   "outputs": [],
   "source": [
    "best_models_backup = [\n",
    "    ('Logistic Regression' , LogisticRegression()),\n",
    "    ('Decision Tree' , DecisionTreeClassifier()),\n",
    "    ('AdaBoost' , AdaBoostClassifier()),\n",
    "    ('GradientBoost' , GradientBoostingClassifier()),\n",
    "    ('RandomForest' , RandomForestClassifier()),\n",
    "    ('XGBoost' , XGBClassifier()),\n",
    "    ('SVC' , SVC()),\n",
    "    ]\n",
    "\n",
    "\n",
    "for key, clf in best_models_backup:\n",
    "    clf.set_params(**best_params_backup[key]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:57:33.895982Z",
     "start_time": "2020-05-31T07:57:33.892025Z"
    }
   },
   "outputs": [],
   "source": [
    "vote_hard_backup = VotingClassifier(estimators = best_models_backup , voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:57:35.379001Z",
     "start_time": "2020-05-31T07:57:35.113744Z"
    }
   },
   "outputs": [],
   "source": [
    "vote_hard_backup.fit(X_trian_backup, y_train)\n",
    "pred_backup = vote_hard_backup.predict(X_trian_backup)\n",
    "cr = classification_report(y_pred=pred_backup, y_true=y_train)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:57:39.050174Z",
     "start_time": "2020-05-31T07:57:39.041669Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred=pred_backup, y_true=y_train)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:58:22.186985Z",
     "start_time": "2020-05-31T07:58:22.156104Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_backup = vote_hard_backup.predict(test_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:59:07.846219Z",
     "start_time": "2020-05-31T07:59:07.841233Z"
    }
   },
   "outputs": [],
   "source": [
    "sum((pred_backup == 1)) / len(pred_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T07:58:47.763569Z",
     "start_time": "2020-05-31T07:58:47.756587Z"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'PassengerId':Test_passengerId, 'Survived':pred_backup.astype(np.int32)})\n",
    "result.to_csv(\"./data/titanic/gender_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "143.75px",
    "left": "1215.75px",
    "right": "20px",
    "top": "305px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
