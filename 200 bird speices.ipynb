{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:42:04.280142Z",
     "start_time": "2020-06-08T15:42:04.256172Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageStat\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet18\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "train_bs = 64\n",
    "test_bs = 64\n",
    "valid_bs = 64\n",
    "lr_init = 0.01\n",
    "max_epoch = 100\n",
    "save_path = '../models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:23:41.432500Z",
     "start_time": "2020-06-08T15:23:41.159428Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = '../200_bird/train/'\n",
    "valid_path = '../200_bird/valid/'\n",
    "test_path = '../200_bird/test/'\n",
    "\n",
    "train_all = {}\n",
    "test_all = {}\n",
    "valid_all = {}\n",
    "\n",
    "image_paths = {\n",
    "    'Train': {},\n",
    "    'Valid': {},\n",
    "    'Test' : {},\n",
    "}\n",
    "\n",
    "\n",
    "categories = os.listdir(train_path)\n",
    "\n",
    "for Type in ['Train', 'Valid', 'Test']:\n",
    "    if Type == 'Train':\n",
    "        root = train_path\n",
    "    elif Type == 'Valid':\n",
    "        root = valid_path\n",
    "    else:\n",
    "        root = test_path\n",
    "    for category in categories:\n",
    "        image_paths[Type][category] = glob.glob(os.path.join(root + category, '*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:23:41.881212Z",
     "start_time": "2020-06-08T15:23:41.772527Z"
    }
   },
   "outputs": [],
   "source": [
    "mapkey = {}\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    mapkey[category] = str(i)\n",
    "\n",
    "def gen_txt(txt_path, image_paths):\n",
    "    f = open(txt_path, 'w')\n",
    "    for key in image_paths.keys():\n",
    "        label = mapkey[key]\n",
    "        for path in image_paths[key]:\n",
    "            line = path + ' ' + label + '\\n'\n",
    "            f.write(line)\n",
    "\n",
    "gen_txt('../200_bird/train.txt', image_paths['Train'])\n",
    "gen_txt('../200_bird/valid.txt', image_paths['Valid'])\n",
    "gen_txt('../200_bird/test.txt', image_paths['Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:27:49.575570Z",
     "start_time": "2020-06-08T15:27:02.401160Z"
    }
   },
   "outputs": [],
   "source": [
    "# 计算训练集的均值和标准差\n",
    "\n",
    "all_paths = []\n",
    "\n",
    "for Type in ['Train', 'Valid', 'Test']:\n",
    "    for category in image_paths[Type]:\n",
    "        for path in image_paths[Type][category]:\n",
    "            all_paths.append(path)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "m_list, s_list = [], []\n",
    "for path in tqdm(all_paths):\n",
    "    img = cv2.imread(path)\n",
    "    img = img / 255.0\n",
    "    m, s = cv2.meanStdDev(img)\n",
    "    m_list.append(m.reshape((3,)))\n",
    "    s_list.append(s.reshape((3,)))\n",
    "m_array = np.array(m_list)\n",
    "s_array = np.array(s_list)\n",
    "m = m_array.mean(axis=0, keepdims=True)\n",
    "s = s_array.mean(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:28:10.243459Z",
     "start_time": "2020-06-08T15:28:10.235325Z"
    }
   },
   "outputs": [],
   "source": [
    "BGR -> RGB\n",
    "print(m[0][::-1])\n",
    "print(s[0][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:41:04.538666Z",
     "start_time": "2020-06-08T15:41:04.515006Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_path, transform=None, target_transform=None):\n",
    "        fh = open(txt_path, 'r')\n",
    "        imgs = []\n",
    "        for line in fh:\n",
    "            line = line.rstrip()\n",
    "            words = line.split(' ')\n",
    "            label = words[-1]\n",
    "            path = ' '.join(words[:-1])\n",
    "            imgs.append((path, int(label)))\n",
    "\n",
    "        self.imgs = imgs        # 最主要就是要生成这个list， 然后DataLoader中给index，通过getitem读取图片数据\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.imgs[index]\n",
    "        img = Image.open(fn).convert('RGB')     # 像素值 0~255，在transfrom.totensor会除以255，使像素值变成 0~1\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # 在这里做transform，转为tensor等等\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:41:14.462553Z",
     "start_time": "2020-06-08T15:41:14.456990Z"
    }
   },
   "outputs": [],
   "source": [
    "normMean = [0.47176269, 0.47058557, 0.40052285]\n",
    "normStd = [0.20032301, 0.196909, 0.20223242]\n",
    "normTransform = transforms.Normalize(normMean, normStd)\n",
    "trainTransform = transforms.Compose([\n",
    "    transforms.RandomCrop(size=(224,224), padding=28),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normTransform,\n",
    "])\n",
    "\n",
    "validTransfrom = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normTransform,\n",
    "])\n",
    "\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normTransform,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:41:24.477437Z",
     "start_time": "2020-06-08T15:41:24.428214Z"
    }
   },
   "outputs": [],
   "source": [
    "train_txt_path = '../200_bird/train.txt'\n",
    "valid_txt_path = '../200_bird/valid.txt'\n",
    "test_txt_path = '../200_bird/test.txt'\n",
    "\n",
    "train_data = MyDataset(txt_path=train_txt_path, transform=trainTransform)\n",
    "valid_data = MyDataset(txt_path=valid_txt_path, transform=trainTransform)\n",
    "test_data = MyDataset(txt_path=test_txt_path, transform=testTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:42:16.928231Z",
     "start_time": "2020-06-08T15:42:16.912157Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_data, batch_size = train_bs, shuffle=True)\n",
    "valid_loader = DataLoader(dataset = valid_data, batch_size = valid_bs, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = test_bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:47:06.964885Z",
     "start_time": "2020-06-08T15:47:05.015800Z"
    }
   },
   "outputs": [],
   "source": [
    "def image_convert(img):\n",
    "    img = img.clone().cpu().numpy()\n",
    "    img = img.transpose(1,2,0)\n",
    "    normStd = [0.22971935, 0.22475049, 0.22525084]\n",
    "    normMean = [0.48827705, 0.45510637, 0.41741   ]\n",
    "    img = img*normStd + normMean\n",
    "    return img\n",
    "\n",
    "\n",
    "def plot_batch():\n",
    "        iter_ = iter(train_loader)\n",
    "        images,labels = next(iter_)\n",
    "        \n",
    "        plt.figure(figsize=(20,10))\n",
    "        for idx in range(train_bs):\n",
    "            plt.subplot(4,train_bs//4,idx+1)\n",
    "            img = image_convert(images[idx])\n",
    "            label = labels[idx]\n",
    "            plt.imshow(img)\n",
    "            plt.title(categories[label])\n",
    "        plt.show()\n",
    "\n",
    "plot_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:49:46.435677Z",
     "start_time": "2020-06-08T15:49:46.419497Z"
    }
   },
   "outputs": [],
   "source": [
    "class fc_part(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(512,200)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:50:00.319750Z",
     "start_time": "2020-06-08T15:49:56.709282Z"
    }
   },
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=False).to(device)\n",
    "model.fc = fc_part().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T13:19:47.532773Z",
     "start_time": "2020-06-05T13:19:44.217381Z"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "if torch.cuda.is_available():\n",
    "    images = images.to(device)\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "comment = f'batch_size{train_bs} lr{lr_init}'\n",
    "tb = SummaryWriter(comment=comment)\n",
    "tb.add_image('images', grid)\n",
    "tb.add_graph(model, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:50:10.768990Z",
     "start_time": "2020-06-08T15:50:10.760389Z"
    }
   },
   "outputs": [],
   "source": [
    "ignored_params = list(map(id, model.fc.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:50:21.269591Z",
     "start_time": "2020-06-08T15:50:21.262028Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([\n",
    "    {'params': base_params},\n",
    "    {'params': model.fc.parameters(), 'lr': lr_init*10}],  lr_init, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()                                                   # 选择损失函数\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)     # 设置学习率下降策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T15:58:17.265361Z",
     "start_time": "2020-06-08T15:50:31.669181Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur_max = 0\n",
    "for epoch in range(1, max_epoch+1):\n",
    "\n",
    "    train_loss = 0.0    # 记录一个epoch的loss之和\n",
    "    train_correct = 0.0\n",
    "    train_total = 0.0\n",
    "    \n",
    "    scheduler.step()  # 更新学习率\n",
    "    \n",
    "    with tqdm(train_loader, desc = f'Train epoch: {epoch}') as t:\n",
    "        model.train()\n",
    "        for data in t:\n",
    "            # 获取图片和标签\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward, backward, update weights\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs, axis = 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += torch.sum(predicted == labels).item()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            #设置进度条右边显示的信息\n",
    "            t.set_postfix(train_loss = loss.item(), train_accuracy = train_correct / train_total)\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        with tqdm(valid_loader, desc = f'Valid epoch: {epoch}') as t:\n",
    "            for data in t:                \n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                    \n",
    "                outputs = model.forward(inputs)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, axis = 1)\n",
    "                valid_total += labels.size(0)\n",
    "                valid_correct += torch.sum(predicted == labels).item()\n",
    "                t.set_postfix(valid_loss = loss.item(), valid_accuracy = valid_correct / valid_total)\n",
    "            \n",
    "    tb.add_scalar('train_loss', train_loss/train_total, epoch)\n",
    "    tb.add_scalar('train_accuracy', train_correct/train_total, epoch)\n",
    "    tb.add_scalar('valid_loss', valid_loss/valid_total, epoch)\n",
    "    tb.add_scalar('valid_accuracy', valid_correct / valid_total, epoch)\n",
    "    \n",
    "    cur_max = max(cur_max, valid_correct / valid_total)\n",
    "    if valid_correct / valid_total >= max(0.95, cur_max):\n",
    "        model_name = 'model_' + str((valid_correct / valid_total)*1000) + '.pkl'\n",
    "        torch.save(model.state_dict(), '../models/' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../models/model_955.0.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    with tqdm(test_loader, desc = f'Test epoch: {1}') as t:\n",
    "        for data in t:                \n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, axis = 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += torch.sum(predicted == labels).item()\n",
    "            t.set_postfix(test_loss = loss.item(), test_accuracy = test_correct / test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T13:37:59.125116Z",
     "start_time": "2020-06-05T13:37:57.290706Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_val_images():\n",
    "\n",
    "    iter_ = iter(test_loader)\n",
    "    images,labels = next(iter_)\n",
    "    images = images.to(device)\n",
    "    pred_labels = labels.to(device)\n",
    "\n",
    "    img_out = model.forward(images)\n",
    "    value, index_val = torch.max(img_out, 1)\n",
    "\n",
    "    # label = label_dict[str(label)]\n",
    "    fig = plt.figure(figsize=(35,9))\n",
    "    for idx in np.arange(10):\n",
    "        ax = fig.add_subplot(2,5,idx+1)\n",
    "        plt.imshow(image_convert(images[idx]))\n",
    "        label = labels[idx]  \n",
    "        pred_label = pred_labels[idx]\n",
    "        ax.set_title('Act {},pred {}'.format(categories[label],categories[pred_label]))\n",
    "        \n",
    "plot_val_images()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.667px",
    "left": "1337.67px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
